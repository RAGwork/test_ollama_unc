{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MP-kswg02pOj"
   },
   "source": [
    "# **Build a simple LLM application with chat models and prompt templates**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ThGBOlgE4ur7"
   },
   "source": [
    "***\n",
    "\n",
    "**Français :** L'objectif est de créer une application simple avec LangChain qui est capable de traduire un texte en anglais vers une autre langue.\n",
    "\n",
    "L'application utilise un seul appel à un modèle de langage (LLM) avec un peu de prompting.\n",
    "\n",
    "***\n",
    "\n",
    "***\n",
    "**English :** The goal is to create a simple app with LangChain that can translate text from English to another language.\n",
    "\n",
    "The app uses a single call to a language model (LLM) with a bit of prompting.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "RWJsIfvPoD9S"
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your LangSmith API key (optional):  ········\n",
      "Enter your LangSmith Project Name (default = \"default\"):  ········\n"
     ]
    }
   ],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "try:\n",
    "    # load environment variables from .env file (requires `python-dotenv`)\n",
    "    from dotenv import load_dotenv\n",
    "\n",
    "    load_dotenv()\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"false\"\n",
    "# os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "\n",
    "if \"LANGSMITH_API_KEY\" not in os.environ:     #connexion au compte LangSmith pour la vérification/test/... du LLM\n",
    "    os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass(\n",
    "        prompt=\"Enter your LangSmith API key (optional): \"\n",
    "    )\n",
    "if \"LANGSMITH_PROJECT\" not in os.environ:    #entrer le nom du projet\n",
    "    os.environ[\"LANGSMITH_PROJECT\"] = getpass.getpass(\n",
    "        prompt='Enter your LangSmith Project Name (default = \"default\"): '\n",
    "    )\n",
    "    if not os.environ.get(\"LANGSMITH_PROJECT\"):   #clé de openAI\n",
    "        os.environ[\"LANGSMITH_PROJECT\"] = \"default\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ollama_api_key = os.environ[\"OLLAMA_API_KEY\"]\n",
    "# print(ollama_api_key)\n",
    "\n",
    "from langchain.chat_models import init_chat_model\n",
    "heders = headers = {\n",
    "'Authorization': f\"Bearer {ollama_api_key}\"\n",
    "}\n",
    "\n",
    "model = init_chat_model(\"llama3.3:70b\", model_provider=\"ollama\", base_url=\"https://ollama.ccad.unc.edu.ar/ollama\", client_kwargs={'headers': headers})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello! How can I assist you today?', additional_kwargs={}, response_metadata={'model': 'llama3.3:70b', 'created_at': '2025-06-10T19:18:39.833884221Z', 'done': True, 'done_reason': 'stop', 'total_duration': 15172080226, 'load_duration': 19860627, 'prompt_eval_count': 11, 'prompt_eval_duration': 1158521069, 'eval_count': 10, 'eval_duration': 13992438854, 'model_name': 'llama3.3:70b'}, id='run--fa38ef38-ea43-4474-9b39-459c09541b03-0', usage_metadata={'input_tokens': 11, 'output_tokens': 10, 'total_tokens': 21})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke(\"Hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xpl2dL_G2b7j",
    "outputId": "7c7acae9-ce05-4906-a66c-fe8e55750e6c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It's nice to meet you. Is there something I can help you with or would you like to chat?\n"
     ]
    }
   ],
   "source": [
    "print(model.invoke(\"Hello\").content) #affiche que la réponse renvoyée par le LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8rcSHoH50FDm"
   },
   "source": [
    "***\n",
    "\n",
    "**Français :** Le message que renvoit **model.invoke (\"Hello, world !\")** est un \"objet complet\" retourné par LangChain. Cet objet contient **la réponse du modèle**(\"Hello! How can I assist you today?\"), mais aussi beaucoup d'autres information techniques comme **le nombre de tokens utilisés** (21 = 11 \"prompt\" + 10 \"réponse\"), **le nom du modèle**, et des détails **pour suivre ou analyser les appels**.\n",
    "\n",
    "Pour juste afficher la réponse du LLM, on peut utiliser \".content\" :\n",
    "\n",
    "==> ***print(model.invoke(\"Hello\").content)***\n",
    "\n",
    "pour ne voir que le texte retrouné à l'utilisateur par le LLM.\n",
    "\n",
    "***\n",
    "***\n",
    "**English :** When you run **model.invoke(\"Hello, world!\")**, LangChain returns a full object. This object includes **the model's response** (for example, \"Hello! How can I assist you today?\"), but also other technical information such as **the number of tokens used** (21 in total: 11 for the prompt and 10 for the response), **the name of the model**, and **some metadata useful for tracking or analyzing the call**.\n",
    "\n",
    "If you only want to display the model's response text, the part the user sees, you can use the .content attribute like this:\n",
    "\n",
    "==> **print(model.invoke(\"Hello\").content)**\n",
    "\n",
    "This will show only the response generated by the LLM, without all the extra information.\n",
    "\n",
    "***\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DMu-TfBM2mtI"
   },
   "source": [
    "---\n",
    "\n",
    "\n",
    "# **USING LANGUAGE MODELS**\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OY2YKKtcHUx6"
   },
   "source": [
    "***\n",
    "\n",
    "**Français :** Dans cet exemple, on apprend à utiliser un modèle de langage avec LangChain. La première étape consiste à installer la bibliothèque LangChain avec le modèle OpenAI.\n",
    "\n",
    "Le modèle de chat dans LangChain est un \"Runnable\", ainsi on peut lui donner en entrée de message? Un exemple de message est créé avec une liste de deux objets : un message système demandant de traduire un texte de l'anglais vers l'italien, et un message utilisateur contenant le texte à traduire (\"hi!\"). On envoit ces messages au modèle via la méthode `.invoke(messages)` pour obtenir une réponse.\n",
    "\n",
    "Le modèle retourne une réponse sous forme de AIMessage, qui contient les informations sur la sortie générée, comme le texte traduit (\"Ciao!\") ainsi que des détails sur l'utilisation des tokens (nombre de tokens utilisés pour la demande et la réponse).\n",
    "\n",
    "Enfin, LangChain prend également en charge l'invocation asynchrone et le streaming, permettant de récupérer les réponses du modèle par tokens individuels. Cela permet de traiter chaque token au fur et à mesure qu'il est généré, ce qui est utile pour des applications où la rapidité de la réponse est cruciale, comme dans les systèmes de chat en temps réel.\n",
    "\n",
    "\n",
    "***\n",
    "***\n",
    "**English :** In this example, we learn how to use a language model with LangChain.\n",
    "The first step is to install the LangChain library along with the OpenAI model.\n",
    "\n",
    "The chat model in LangChain is a Runnable, so we can give it a message input.\n",
    "An example message is created using a list of two objects:\n",
    "a system message asking to translate text from English to Italian,\n",
    "and a user message containing the text to translate (\"hi!\").\n",
    "We send these messages to the model using the .invoke(messages) method to get a response.\n",
    "\n",
    "The model returns a response as an AIMessage, which contains information about the generated output,\n",
    "such as the translated text (\"Ciao!\") and details about token usage\n",
    "(the number of tokens used for the prompt and the response).\n",
    "\n",
    "Finally, LangChain also supports asynchronous invocation and streaming,\n",
    "which allows us to receive the model’s response token by token.\n",
    "This is useful for applications where response speed is important,\n",
    "such as in real-time chat systems.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7fq4sol486Yt"
   },
   "source": [
    "***\n",
    "**Translation : english ---> italian**\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SCMy5kRx5hWZ",
    "outputId": "1e317578-be41-4594-8053-0d37de2dc48d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Ciao!', additional_kwargs={}, response_metadata={'model': 'llama3.3:70b', 'created_at': '2025-06-10T20:28:13.787389555Z', 'done': True, 'done_reason': 'stop', 'total_duration': 13542526897, 'load_duration': 3131487071, 'prompt_eval_count': 24, 'prompt_eval_duration': 5238506500, 'eval_count': 4, 'eval_duration': 5169308708, 'model_name': 'llama3.3:70b'}, id='run--0da6352f-f5dc-47bc-9f9b-e10fdc4a577a-0', usage_metadata={'input_tokens': 24, 'output_tokens': 4, 'total_tokens': 28})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Traduction anglais ---> iatlien\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(\"Translate the following from English into Italian\"),\n",
    "    HumanMessage(\"hi!\"),\n",
    "]\n",
    "\n",
    "model.invoke(messages) #objet complet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Edo9_zh15nrK",
    "outputId": "4b4b2ec4-388c-4b03-dbdd-7a37e8a85b90"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ciao!\n"
     ]
    }
   ],
   "source": [
    "print(model.invoke(messages).content) #affiche ce qui est renvoyé à l'utilisateur"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ncMqnlyN9IbQ"
   },
   "source": [
    "***\n",
    "**Translation : english ---> chinese**\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rHUu0LUj8Qnz",
    "outputId": "f8509032-a046-4546-e89a-f798f11f78d5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='你好！（nǐ hǎo）', additional_kwargs={}, response_metadata={'model': 'llama3.3:70b', 'created_at': '2025-06-10T20:28:53.715889725Z', 'done': True, 'done_reason': 'stop', 'total_duration': 21671275011, 'load_duration': 19519990, 'prompt_eval_count': 24, 'prompt_eval_duration': 1991469224, 'eval_count': 11, 'eval_duration': 19658882093, 'model_name': 'llama3.3:70b'}, id='run--d144861b-2714-4a1a-b2a7-f7a037b1d42a-0', usage_metadata={'input_tokens': 24, 'output_tokens': 11, 'total_tokens': 35})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Traduction anglais ---> chinois\n",
    "messages_bis = [\n",
    "    SystemMessage(\"Translate the following from English into Chinese\"),\n",
    "    HumanMessage(\"hi!\"),\n",
    "]\n",
    "\n",
    "model.invoke(messages_bis) #objet complet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "39d4chQE8uln",
    "outputId": "ec0b7355-07b6-4f25-db33-3b2bd218eb25"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "！(nǐ hǎo)\n"
     ]
    }
   ],
   "source": [
    "print(model.invoke(messages_bis).content) #affiche ce qui est renvoyé à l'utilisateur"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n2aE44rX8AWa"
   },
   "source": [
    "***\n",
    "**Français :** On remarque que la traduction a bien été exécutée pour les deux langues, l'italien et le chinois.\n",
    "\n",
    "***\n",
    "***\n",
    "**English :** We can see that the translation was successfully done for both languages: Italian and Chinese.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RhCgFHoI66de",
    "outputId": "a0a18cc1-c0a8-4ef9-fb73-6634811a4547"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hello! It's nice to meet you. Is there something I can help you with or would you like to chat?\", additional_kwargs={}, response_metadata={'model': 'llama3.3:70b', 'created_at': '2025-06-10T20:31:21.834778045Z', 'done': True, 'done_reason': 'stop', 'total_duration': 46872452948, 'load_duration': 20004653, 'prompt_eval_count': 11, 'prompt_eval_duration': 1081495896, 'eval_count': 25, 'eval_duration': 45769958153, 'model_name': 'llama3.3:70b'}, id='run--cdaae87e-c5d3-47e3-add3-2dd043657bab-0', usage_metadata={'input_tokens': 11, 'output_tokens': 25, 'total_tokens': 36})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Exemples de 3 façons différentes d'appeler \"model.invoke\"\n",
    "\n",
    "model.invoke(\"Hello\") #simple chaîne de texte\n",
    "\n",
    "model.invoke([{\"role\": \"user\", \"content\": \"Hello\"}]) #format structuré avec un dictionnaire\n",
    "\n",
    "model.invoke([HumanMessage(\"Hello\")]) #format orienté objet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LfS0nqH869fI",
    "outputId": "4e1aa6fa-0497-4581-b17e-1d36c28612d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It's nice to meet you. Is there something I can help you with or would you like to chat?\n",
      "Hello! It's nice to meet you. Is there something I can help you with or would you like to chat?\n",
      "Hello! How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "print(model.invoke(\"Hello\").content)\n",
    "\n",
    "print(model.invoke([{\"role\": \"user\", \"content\": \"Hello\"}]).content)\n",
    "\n",
    "print(model.invoke([HumanMessage(\"Hello\")]).content)\n",
    "\n",
    "###Les trois appels renvoient donc bien la même chose###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ji0HtbcMDmXg"
   },
   "source": [
    "***\n",
    "**Français :** **Le streaming** permet de recevoir les réponses du modèle de manière progressive, token par token.\n",
    "\n",
    "Étant donné que les modèles de chat sont des \"Runnables\" (objets exécutables), ils offrent une interface standard qui supporte l'exécution **asynchrone** et **le streaming**, ce qui permet d'obtenir chaque token **au fur et à mesure de la génération de la réponse**.\n",
    "\n",
    "***\n",
    "***\n",
    "**English :** **Streaming** allows us to receive the model’s responses gradually, token by token.\n",
    "\n",
    "Since chat models are Runnables (executable objects), they provide a standard interface that supports **asynchronous** execution and **streaming**, which makes it possible to get each token as **the response is being generated**.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s_gyzj-5EAdX",
    "outputId": "fb56d4db-5fe1-4679-e6ba-524448aa16d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C|iao|!||"
     ]
    }
   ],
   "source": [
    "for token in model.stream(messages):\n",
    "    print(token.content, end=\"|\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fFvWFah53hym"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "# **PROMPT TEMPLATES**\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AS80LWfdFcNg"
   },
   "source": [
    "***\n",
    "**Français :** Ici, on crée un modèle de prompt pour formater les entrées utilisateur avant de les envoyer au modèle de langage.\n",
    "\n",
    "Tout d'abord, on définit une **variable \"system_template\"** pour donner une instruction générale au modèle. Cette instruction est de traduire un texte de l'anglais vers une langue spécifiée par la variable {language}.\n",
    "\n",
    "Ensuite, un **objet \"ChatPromptTemplate\"** est créé avec deux parties :\n",
    "*   **le message système :** est défini par le **\"system_template\"** ---> indique la tâche (traduire vers la langue cible)\n",
    "*   **le message utilisateur :** est défini par une variable {text} ---> indique le texte à traduire\n",
    "\n",
    "En utilisant **ChatPromptTemplate.from_messages()**, on assemble ces deux messages sous forme de modèle de prompt.\n",
    "\n",
    "Enfin, en appelant **prompt_template.invoke()** avec un dictionnaire contenant les valeurs de la langue (\"Italian\") et du texte (\"hi!\"), ces valeurs sont injectées dans le modèle de prompt, ce qui génère un message final prêt à être envoyé au modèle de langage pour effectuer la traduction.\n",
    "\n",
    "Ainsi, on transforme **les entrées utilisateur brutes** en un format **structuré**, **facile à traiter** par le modèle de langage.\n",
    "\n",
    "\n",
    "***\n",
    "***\n",
    "**English :** Here, we create a prompt template to format user inputs before sending them to the language model.\n",
    "\n",
    "First, we define a **variable \"system_template\"** to give a general instruction to the model.\n",
    "This instruction is to translate a text from English to a language specified by the variable {language}.\n",
    "\n",
    "Next, a ChatPromptTemplate object is created with two parts:\n",
    "* **the system message:** defined by the system_template ---> it tells the task (translate to the target language)\n",
    "* **the user message:** defined by a variable {text} ---> it shows the text to translate\n",
    "\n",
    "Using **ChatPromptTemplate.from_messages()**, we combine these two messages into one prompt template.\n",
    "\n",
    "Finally, by calling **prompt_template.invoke()** with a dictionary containing the language (\"Italian\") and the text (\"hi!\"), these values are inserted into the prompt.\n",
    "This produces a final message ready to be sent to the language model for translation.\n",
    "\n",
    "So, we turn raw user inputs into a structured format that is easy for the language model to handle.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "1a3W08ZNEiOk"
   },
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "system_template = \"Translate the following from English into {language}\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages([(\"system\", system_template), (\"user\", \"{text}\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fdyu4LcpEoxH",
    "outputId": "854b9d99-4532-403e-bcfb-9a29ecc12765"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='Translate the following from English into Italian', additional_kwargs={}, response_metadata={}), HumanMessage(content='hi!', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = prompt_template.invoke({\"language\": \"Italian\", \"text\": \"hi!\"})\n",
    "\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tABH06wgEsd7",
    "outputId": "87b1103d-e227-452f-edb3-d796cd5162e8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='Translate the following from English into Italian', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='hi!', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt.to_messages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zlLX9SuUEvon",
    "outputId": "fae23469-312f-4f5c-8d68-0f45a4b1d4d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ciao!\n"
     ]
    }
   ],
   "source": [
    "response = model.invoke(prompt)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
