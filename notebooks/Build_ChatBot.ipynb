{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-IEAg7f2QnvL"
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# **Build a Chatbot**\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GHXnZA4NQ5F_"
   },
   "source": [
    "***\n",
    "**Français :** L'objectif est de créer un chatbot intelligent en Python qui est capable de tenir une conversation fluide avec mémoire. Pour cela, on utilisera LangChain v0.3 et LangGraph. Il faudra également permettre la personnalisation du comportement via des prompts dynamiques.\n",
    "\n",
    "***\n",
    "***\n",
    "**English :** The goal is to create an intelligent chatbot in Python that can hold a smooth conversation with memory. To do this, we will use LangChain v0.3 and LangGraph. We will also need to allow behavior customization through dynamic prompts.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NPsI5BI69uMM"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "# **QUICK START**\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "XoMJT7XU93ld"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.chat_models import init_chat_model\n",
    "from dotenv import load_dotenv\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_core.globals import set_llm_cache\n",
    "from langchain_core.caches import InMemoryCache\n",
    "\n",
    "set_llm_cache(InMemoryCache())\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "ollama_api_key = os.environ[\"OLLAMA_API_KEY\"]\n",
    "\n",
    "heders = headers = {\n",
    "'Authorization': f\"Bearer {ollama_api_key}\"\n",
    "}\n",
    "\n",
    "model = init_chat_model(\"llama3.3:70b\", model_provider=\"ollama\", base_url=\"https://ollama.ccad.unc.edu.ar/ollama\",\n",
    "                        client_kwargs={'headers': headers})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6Lcw-DAm-HIF",
    "outputId": "ad4cb877-8e5b-4e55-ac1d-c4a326a3b844"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hello Bob! It's nice to meet you. Is there something I can help you with or would you like to chat?\", additional_kwargs={}, response_metadata={'model': 'llama3.3:70b', 'created_at': '2025-06-10T21:10:13.782283039Z', 'done': True, 'done_reason': 'stop', 'total_duration': 48015950925, 'load_duration': 20477560, 'prompt_eval_count': 15, 'prompt_eval_duration': 2734006868, 'eval_count': 26, 'eval_duration': 45260409671, 'model_name': 'llama3.3:70b'}, id='run--ea15f658-f0a0-460b-aefb-f84db7ade4b5-0', usage_metadata={'input_tokens': 15, 'output_tokens': 26, 'total_tokens': 41})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "model.invoke([HumanMessage(content=\"Hi! I'm Bob\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e_JZSedE-QDp",
    "outputId": "4a46b58c-37c5-4d8b-e778-2d5feefefd5e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Bob! It's nice to meet you. Is there something I can help you with or would you like to chat?\n"
     ]
    }
   ],
   "source": [
    "print(model.invoke([HumanMessage(content=\"Hi! I'm Bob\")]).content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D9xFbrDl-OKl",
    "outputId": "a61f2315-7432-4f7b-dc65-b6c1fdc65648"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"I don't know your name. I'm a large language model, I don't have any information about you or your personal details. Each time you interact with me, it's a new conversation and I don't retain any context or information from previous conversations. If you'd like to share your name with me, I'd be happy to chat with you!\", additional_kwargs={}, response_metadata={'model': 'llama3.3:70b', 'created_at': '2025-06-10T21:12:43.511477989Z', 'done': True, 'done_reason': 'stop', 'total_duration': 127505335315, 'load_duration': 19263574, 'prompt_eval_count': 15, 'prompt_eval_duration': 3390411150, 'eval_count': 73, 'eval_duration': 124094996114, 'model_name': 'llama3.3:70b'}, id='run--7d688936-fe79-4ed8-bfad-19d536b0d086-0', usage_metadata={'input_tokens': 15, 'output_tokens': 73, 'total_tokens': 88})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke([HumanMessage(content=\"What's my name?\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3B2mu5HP_DST",
    "outputId": "f49be8bd-882d-46ed-e357-c67d7fea9539"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't know your name. I'm a large language model, I don't have any information about you or your personal details. Each time you interact with me, it's a new conversation and I don't retain any context or information from previous conversations. If you'd like to share your name with me, I'd be happy to chat with you!\n"
     ]
    }
   ],
   "source": [
    "print(model.invoke([HumanMessage(content=\"What's my name?\")]).content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hUIo2wQlVSwI"
   },
   "source": [
    "***\n",
    "\n",
    "**Français :** On remarque ici, que le model seul ne possède pas de mémoire. Il est incapable de retrouver le nom qui a été donné à la question précédante.\n",
    "\n",
    "***\n",
    "***\n",
    "**English :** Here, we notice that the model alone does not have memory.\n",
    "It cannot remember the name given to the previous question.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o0nPASU5_Oql",
    "outputId": "8d06e46b-6e5f-414a-f7e3-4964ee03a217"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Your name is Bob! You told me that when we started chatting.', additional_kwargs={}, response_metadata={'model': 'llama3.3:70b', 'created_at': '2025-06-10T21:39:25.294352029Z', 'done': True, 'done_reason': 'stop', 'total_duration': 15003876117, 'load_duration': 3450937216, 'prompt_eval_count': 40, 'prompt_eval_duration': 3214802017, 'eval_count': 15, 'eval_duration': 8335553423, 'model_name': 'llama3.3:70b'}, id='run--bf90c057-53d2-47ab-8d55-5e2f3b2e1f8c-0', usage_metadata={'input_tokens': 40, 'output_tokens': 15, 'total_tokens': 55})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage\n",
    "\n",
    "model.invoke(\n",
    "    [\n",
    "        HumanMessage(content=\"Hi! I'm Bob\"),\n",
    "        AIMessage(content=\"Hello Bob! How can I assist you today?\"),\n",
    "        HumanMessage(content=\"What's my name?\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pV8Zdi-JVgiR",
    "outputId": "c4c2bf07-1332-4474-8088-43183f366a18"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your name is Bob! You told me that when we started chatting.\n"
     ]
    }
   ],
   "source": [
    "print(model.invoke(\n",
    "    [\n",
    "        HumanMessage(content=\"Hi! I'm Bob\"),\n",
    "        AIMessage(content=\"Hello Bob! How can I assist you today?\"),\n",
    "        HumanMessage(content=\"What's my name?\"),\n",
    "    ]\n",
    ").content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6JYAR27rWHgB"
   },
   "source": [
    "***\n",
    "\n",
    "**Français :** Quand on met dans le même message toutes les questions, alors le chat est capable de renvoyer le nom de l'utilisateur. Mais ce n'est pas pratique, car l'utilisateur ne va pas faire une liste de toutes ses informations pour poser sa question. Il faut que le chatbot soit capable de mémoriser les informations envoyées par l'utilisateur.\n",
    "\n",
    "***\n",
    "***\n",
    "**English :** When we put all the questions in the same message, the chat can return the user’s name. But this is not practical, because the user will not list all their information every time they ask a question. The chatbot needs to be able to remember the information the user has given.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xfkeR2Yn_deE"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "# **PERTINANCE DES MESSAGES**\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "itYdyTN9_ad6"
   },
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import START, MessagesState, StateGraph\n",
    "\n",
    "# Define a new graph\n",
    "workflow = StateGraph(state_schema=MessagesState)\n",
    "\n",
    "\n",
    "# Define the function that calls the model\n",
    "def call_model(state: MessagesState):\n",
    "    response = model.invoke(state[\"messages\"])\n",
    "    return {\"messages\": response}\n",
    "\n",
    "\n",
    "# Define the (single) node in the graph\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "\n",
    "# Add memory\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "frojBsw-4Dce"
   },
   "source": [
    "***\n",
    "**Français :** On définit un pointeur de contrôle pour mémoriser la conversation, avec : \"MemorySaver\".\n",
    "\n",
    "***\n",
    "***\n",
    "**English :** We define a checkpointer to save the conversation, called \"MemorySaver\".\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "M9-yZMpi_xvi"
   },
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc123\"}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dutwVAeD4T2l"
   },
   "source": [
    "***\n",
    "**Français :** On utilise \"thread_id\" comme identifiant de la conversation. Ainsi, plusieurs utilisateurs peuvent utiliser simultanément le chatbot.\n",
    "\n",
    "***\n",
    "***\n",
    "**English :** We use \"thread_id\" as the conversation identifier. This way, multiple users can use the chatbot at the same time.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-UK8CD7h_2QA",
    "outputId": "0deeeb97-76e6-4734-a9b8-dce663e05b6c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hello Bob! It's nice to meet you. Is there something I can help you with or would you like to chat?\n"
     ]
    }
   ],
   "source": [
    "query = \"Hi! I'm Bob.\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()  # output contains all messages in state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZdEFO0zR_6kN",
    "outputId": "dfbfb949-ba41-451e-81bb-5f8f7f501b89"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Your name is Bob! You told me that when we started chatting.\n"
     ]
    }
   ],
   "source": [
    "query = \"What's my name?\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lUcZQy-t4oKG"
   },
   "source": [
    "***\n",
    "**Français :** On remarque ainsi que le chatbot a ici mémorisé le nom de l'utilisateur sachant qu'il n'a pas été donnée dans le même message.\n",
    "\n",
    "***\n",
    "***\n",
    "**English :** We see that the chatbot has remembered the user’s name here, even though it was not given in the same message.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PIFdEMaW__tW",
    "outputId": "2524f339-feb3-4628-cb3f-cdb7bf0fb8b8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "I don't know your name. I'm a large language model, I don't have the ability to retain information about individual users or their personal details, including names. Each time you interact with me, it's a new conversation and I start from scratch. If you'd like to share your name with me, I'd be happy to chat with you!\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc234\"}}\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yZF1xYi343rE"
   },
   "source": [
    "***\n",
    "\n",
    "**Français :** On remarque aussi que si on change d'identifiant, le chat bot n'est plus capable de répondre à la question. Ainsi, plusieurs utilisateurs peuvent donc utiliser le chatbot simultanément.\n",
    "\n",
    "***\n",
    "***\n",
    "**English :** We also notice that if we change the identifier, the chatbot can no longer answer the question. This way, multiple users can use the chatbot at the same time.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uwzLDeSoAC06",
    "outputId": "7ea7af9c-be53-493c-b024-c828a3372fe0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "I remember! Your name is Bob.\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc123\"}}\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TpnKQVz55N-I"
   },
   "source": [
    "***\n",
    "**Français :** Si on repose la question du nom avec le bon identifiant de conversation, le chatbot est à nouveau capable de retrouver la réponse.\n",
    "\n",
    "La conversion est donc bien mémorisée au bon endroit.\n",
    "\n",
    "***\n",
    "***\n",
    "**English :** If we ask the name question again with the correct conversation identifier, the chatbot is able to find the answer again.\n",
    "\n",
    "The conversation is thus properly saved in the right place.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lyhj0_bFAKBJ"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "# **PROMPT TEMPLATES**\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "CjEnN0Ucq8N3"
   },
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You talk like a pirate. Answer all questions to the best of your ability.\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "XGRGVaVYzhV3"
   },
   "outputs": [],
   "source": [
    "workflow = StateGraph(state_schema=MessagesState)\n",
    "\n",
    "\n",
    "def call_model(state: MessagesState):\n",
    "    prompt = prompt_template.invoke(state)\n",
    "    response = model.invoke(prompt)\n",
    "    return {\"messages\": response}\n",
    "\n",
    "\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YsixDct97_LW"
   },
   "source": [
    "***\n",
    "**Français :** On ajoute des modèles d'invite pour transformer le texte en langage brut envoyé par l'utilisateur en texte utilisable par le LLM. Pour celà, on crée un \"ChatPromptTemplate\" grâce à \"MessagPlaceholder\".\n",
    "\n",
    "On a plus qu'à mettre à jour la fonction qui appelle notre modèle.\n",
    "\n",
    "***\n",
    "***\n",
    "**English :** We add prompt templates to transform the raw text sent by the user into text usable by the LLM. To do this, we create a \"ChatPromptTemplate\" using \"MessagePlaceholder\". Then, we just need to update the function that calls our model.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aKDP-UIjzmCx",
    "outputId": "5bc7978a-fabd-4162-91f0-ce4d6127aeb0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Ahoy, Jim me lad! 'Tis a grand day to be havin' a chat, don't ye think? What brings ye to these fair waters? Are ye lookin' fer treasure, or just passin' the time with a swashbucklin' pirate like meself?\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc345\"}}\n",
    "query = \"Hi! I'm Jim.\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CxDAWtrFjOZD"
   },
   "source": [
    "***\n",
    "**Français :** On remarque que la demande de l'utilisateur a été respecté. En effet, ici le model répond en langage pirate.\n",
    "\n",
    "***\n",
    "***\n",
    "**English :** We notice that the user’s request was followed. Indeed, here the model responds in pirate language.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NIH9W_bVzsHT",
    "outputId": "6923f54b-a1b6-47a7-9f51-262bfb1f70cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Yer name be Jim, me hearty! I remember ye introducin' yerself to me just a moment ago. Yer a landlubber with a fine moniker, if I do say so meself!\n"
     ]
    }
   ],
   "source": [
    "query = \"What is my name?\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "--NW6LoN88gz"
   },
   "source": [
    "***\n",
    "**Français :** Le modèle fonctionne toujours aussi bien puisqu'il se souvient du nom et parle toujours en langage pirate.\n",
    "\n",
    "***\n",
    "***\n",
    "**English :** The model still works well because it remembers the name and still speaks in pirate language.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "OkbOCCCnztf8"
   },
   "outputs": [],
   "source": [
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant. Answer all questions to the best of your ability in {language}.\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "VzC0NRY9z0Hi"
   },
   "outputs": [],
   "source": [
    "from typing import Sequence\n",
    "\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langgraph.graph.message import add_messages\n",
    "from typing_extensions import Annotated, TypedDict\n",
    "\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]\n",
    "    language: str\n",
    "\n",
    "\n",
    "workflow = StateGraph(state_schema=State)\n",
    "\n",
    "\n",
    "def call_model(state: State):\n",
    "    prompt = prompt_template.invoke(state)\n",
    "    response = model.invoke(prompt)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l_LIPEQp9f8p"
   },
   "source": [
    "***\n",
    "**Français :** On complexifie un peu le prompt en ajoutant un nouveau paramètre qui permet de personnaliser davantage la réponse. On ajoute le choix de la langue de la réponse.\n",
    "\n",
    "***\n",
    "***\n",
    "**English :** We make the prompt a bit more complex by adding a new parameter that allows more customization of the response. We add the choice of the response language.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lzeEugblz1f_",
    "outputId": "403caa9c-7955-4f1d-d6cf-698e6ac7eb01"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "¡Hola Bob! Encantado de conocerte. ¿En qué puedo ayudarte hoy?\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc456\"}}\n",
    "query = \"Hi! I'm Bob.\"\n",
    "language = \"Spanish\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke(\n",
    "    {\"messages\": input_messages, \"language\": language},\n",
    "    config,\n",
    ")\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Vd5GP7ZAz5NT",
    "outputId": "c9760001-ec41-42db-fc07-d7005dda665b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Tu nombre es Bob. Me lo dijiste cuando te presentaste. ¿Te acuerdas?\n"
     ]
    }
   ],
   "source": [
    "query = \"What is my name?\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke(\n",
    "    {\"messages\": input_messages},\n",
    "    config,\n",
    ")\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h61ThtAT93yn"
   },
   "source": [
    "***\n",
    "**Français :** On remarque que tout fonction.\n",
    "\n",
    "Le chatbot est capable de retrouver son prénom même si l'information n'est pas dans le même message.\n",
    "\n",
    "Le chatbot répond dans le langue choisie par l'utilisateur, même si celle-ci diffère de la langue du message envoyé par l'utilisateur.\n",
    "\n",
    "***\n",
    "***\n",
    "**English :** We notice that everything works.\n",
    "\n",
    "The chatbot can remember its name even if the information is not in the same message.\n",
    "\n",
    "The chatbot responds in the language chosen by the user, even if it is different from the language of the user’s message.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FJEbpVc60AUU"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "# **MANAGING CONVERSATION HISTORY**\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BRGQzrC-z8b3",
    "outputId": "94b9370d-037b-4f81-aa8c-6db0e3b85078"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8650126dc2d4f399436d7f08b9da61e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "963077e7c39a4aa0978067e76e672889",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7574af288f8344669bd069e3e37b3234",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "037eb1fc103a4283bce3558305e95f7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15aa456d40e04f20b390561fa209d825",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content=\"you're a good assistant\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content=\"hi! I'm bob\", additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='hi!', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='I like vanilla ice cream', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='nice', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='whats 2 + 2', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='4', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='thanks', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='no problem!', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='having fun?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='yes!', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import SystemMessage, trim_messages\n",
    "\n",
    "trimmer = trim_messages(\n",
    "    max_tokens=65,\n",
    "    strategy=\"last\",\n",
    "    token_counter=model,\n",
    "    include_system=True,\n",
    "    allow_partial=False,\n",
    "    start_on=\"human\",\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"you're a good assistant\"),\n",
    "    HumanMessage(content=\"hi! I'm bob\"),\n",
    "    AIMessage(content=\"hi!\"),\n",
    "    HumanMessage(content=\"I like vanilla ice cream\"),\n",
    "    AIMessage(content=\"nice\"),\n",
    "    HumanMessage(content=\"whats 2 + 2\"),\n",
    "    AIMessage(content=\"4\"),\n",
    "    HumanMessage(content=\"thanks\"),\n",
    "    AIMessage(content=\"no problem!\"),\n",
    "    HumanMessage(content=\"having fun?\"),\n",
    "    AIMessage(content=\"yes!\"),\n",
    "]\n",
    "\n",
    "trimmer.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "m-BQxbQ40Qvw"
   },
   "outputs": [],
   "source": [
    "workflow = StateGraph(state_schema=State)\n",
    "\n",
    "\n",
    "def call_model(state: State):\n",
    "    trimmed_messages = trimmer.invoke(state[\"messages\"])\n",
    "    prompt = prompt_template.invoke(\n",
    "        {\"messages\": trimmed_messages, \"language\": state[\"language\"]}\n",
    "    )\n",
    "    response = model.invoke(prompt)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e7D_pwSnD0ya"
   },
   "source": [
    "***\n",
    "**Français :** On ajoute une étape dans notre code, après avoir chargé les messages précédents et avant le modèle de l'invite. Ainsi, on peut libérer de l'espace mémoire.\n",
    "\n",
    "Le \"Trimmer\" permet de spécifier le nombre de tokens que nous voulons conserver et beaucoup d'autres paramètres.\n",
    "\n",
    "***\n",
    "***\n",
    "**English :** We add a step in our code, after loading the previous messages and before the prompt model. This way, we can free up memory space.\n",
    "\n",
    "The \"Trimmer\" lets us specify the number of tokens we want to keep and many other settings.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XRPaoCS60VCl",
    "outputId": "8e0186c3-108a-4dbf-f4f6-0fef480e1ad8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Your name is Bob. We established that earlier in our conversation!\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc567\"}}\n",
    "query = \"What is my name?\"\n",
    "language = \"English\"\n",
    "\n",
    "input_messages = messages + [HumanMessage(query)]\n",
    "output = app.invoke(\n",
    "    {\"messages\": input_messages, \"language\": language},\n",
    "    config,\n",
    ")\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gy_bCTa7DroN"
   },
   "source": [
    "***\n",
    "**Français :** On remarque qu'il ne se souveint plus du nom, qui a été retiré de la mémoire pour gain de place.\n",
    "\n",
    "***\n",
    "***\n",
    "**English :** We notice that it no longer remembers the name, which was removed from memory to save space.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U946FWsU0ZMc",
    "outputId": "77195d56-d4e8-4093-e7e7-f5979cdc7042"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "You asked what 2 + 2 is, and the answer was 4.\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc678\"}}\n",
    "query = \"What math problem did I ask?\"\n",
    "language = \"English\"\n",
    "\n",
    "input_messages = messages + [HumanMessage(query)]\n",
    "output = app.invoke(\n",
    "    {\"messages\": input_messages, \"language\": language},\n",
    "    config,\n",
    ")\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WZuueW1GDCxJ"
   },
   "source": [
    "***\n",
    "**Français :** Cependant, on remarque qu'il se souvient cependant du problème de maths posé.\n",
    "\n",
    "***\n",
    "***\n",
    "**English :** However, we notice that it still remembers the math problem that was asked.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_2xj5C2n0dHf"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "# **STREAMING**\n",
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zHuyErrM0g9i",
    "outputId": "734e2591-a108-4e5d-88a8-bba96754a4cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello| Todd|!| Here|'s| one| for| you|:| Why| couldn|'t| the| bicycle| stand| up| by| itself|?| Because| it| was| two|-t|ired|!| Hope| that| made| you| smile|!| Do| you| want| to| hear| another| one|?||"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc789\"}}\n",
    "query = \"Hi I'm Todd, please tell me a joke.\"\n",
    "language = \"English\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "for chunk, metadata in app.stream(\n",
    "    {\"messages\": input_messages, \"language\": language},\n",
    "    config,\n",
    "    stream_mode=\"messages\",\n",
    "):\n",
    "    if isinstance(chunk, AIMessage):  # Filter to just model responses\n",
    "        print(chunk.content, end=\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
